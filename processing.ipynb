{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "import sys\n",
    "import sentence_transformers\n",
    "\n",
    "\n",
    "# download intfloat/multilingual-e5-base\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 418/418 [00:00<00:00, 1.04MB/s]\n",
      "sentencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 176MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.1M/17.1M [00:00<00:00, 374MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 280/280 [00:00<00:00, 2.66MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/tokenizers/multilingual-e5-base/tokenizer_config.json',\n",
       " 'models/tokenizers/multilingual-e5-base/special_tokens_map.json',\n",
       " 'models/tokenizers/multilingual-e5-base/sentencepiece.bpe.model',\n",
       " 'models/tokenizers/multilingual-e5-base/added_tokens.json',\n",
       " 'models/tokenizers/multilingual-e5-base/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from mteb import MTEB\n",
    "from tasks import *\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# load from disk\n",
    "class embedding_model:\n",
    "    def __init__(self, model, tokenizer, device, inference=False):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        if inference:\n",
    "            self.model.eval()\n",
    "            self.model.requires_grad_(False)\n",
    "\n",
    "    def average_pool(last_hidden_states: torch.Tensor,\n",
    "                 attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    \n",
    "    def encode(self, sentences, batch_size=512):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch_sentences = sentences[i:i+batch_size]\n",
    "            embedding = self.__call__(batch_sentences)\n",
    "            # Add each sentence embedding to the list as a separate ndarray\n",
    "            embeddings.extend(embedding.detach().cpu().numpy())\n",
    "        return embeddings\n",
    "\n",
    "    def __call__(self, data):\n",
    "        tokens_and_mask = self.tokenizer(data, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        model_output = self.model(tokens_and_mask[\"input_ids\"], attention_mask=tokens_and_mask[\"attention_mask\"])\n",
    "        embedding = embedding_model.average_pool(model_output.last_hidden_state, attention_mask=tokens_and_mask[\"attention_mask\"])\n",
    "        # normalize the embedding\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        return embedding    \n",
    "\n",
    "a = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-base\")\n",
    "# save a to models/raw_models/multilingual-e5-base\n",
    "save_path = \"models/tokenizers/multilingual-e5-base\"\n",
    "a.save_pretrained(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
